---
title: "Online News Popularity - Analysis"
author: "cbcoursera1"
date: "Monday, June 29, 2015"
output: html_document
---

```{r}

# Code allowing for figures dynamically in Appendix

library(knitr)
opts_chunk$set(echo = F, warnings = F, message = F, comment = F, include = T, cache = T)

```

```{r}

# Report format

# indicates a comment (ie code section header)
## indicates a subcomment (comments within code section )
### indicates a commented piece of code - eg a function you tried but doesn't contribute further down the line

# During exploratory analysis let's keep our code chunks separate

# Feel free to add some formatting advice here

```

```{r, 1}

# Libraries

install.packages('data.table')
library(data.table)
library(ggplot2)
library(reshape2)

# Ingest the data

download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip", "OnlineNewsPopularity.zip")
unzip("OnlineNewsPopularity.zip")
onp <- fread("OnlineNewsPopularity/OnlineNewsPopularity.csv", sep = ",", header = T, data.table = T)

onp.dt <- fread("OnlineNewsPopularity/OnlineNewsPopularity.csv", sep = ",", header = T, data.table = F)

pop<-read.csv('/home/homeboy/Documents/Coursera/statsLearning/groupProjects/OnlineNewsPopularity/OnlineNewsPopularity/OnlineNewsPopularity.csv')

onp.bak <- onp # backup
onp <- onp.bak # revert from backup
### lapply(mean, onp) # quick visual check that means match summary provided with data

# Clean data

setnames(onp, gsub("_", ".", names(onp))) # fix column titles
### lapply(onp, class) # look at the classes, mostly they look fine
### onp[, grep(".is.", names(onp))] <- lapply(onp[, grep(".is.", names(onp)), with = F], as.logical)

```

=========================================================================

# Everything after here is WIP

=========================================================================

```{r}

## Let's do three things with the data

## turn the weekday.is.monday etc variables into a factor variable of dayofweek

onp[, day.of.week := factor(rep("", nrow(onp)), levels = c("mon", "tue", "wed", "thu", "fri", "sat", "sun"))]  ## create empty factor variable

dow <- data.table("s" = c("mon", "tue", "wed", "thu", "fri", "sat", "sun"), "l" = c("monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday")) # create data table of short and long day names

## CB ===================================================================
## Having a hard time with this. My goal is to binary search the data.table in
## each weekday.is column, and if it's a 1 (ie TRUE) then set the day.of.week
## column with the corresponding factor
## My plan was to iterate through the for statement for each pair, eg "mon", "monday"
## The problem right now is that I can't get a simple binary search to work
## on the data.table, eg    
##
## onp[weekday.is.monday == 1]
##
## it tells me the object is not found, but it is the column name. Removing the
## .s also does not help. However
##
## onp[url == 1]
##
## works fine. Maybe because it's the first column? Anyway, help appreciated.

for (i in nrow(dow)) {
    
    onp[paste0("weekday.is.", dow[i, l]) == T, day.of.week := dow[i, s]]

}   ## iterate across days of week filling the day.of.week column

## turn the data.channel.is.bus etc variables into a factor varaible of channel (same method as dow)

## grab the date from the URL
```

Let's have a look over the distribution of shares:
```{r}
ggplot(data=pop[pop$shares<13000,], aes(x=shares))+geom_histogram(binwidth=I(13000/30), color="orange")

ggplot(data=pop[pop$shares<13000,], aes(x=shares))+geom_histogram(aes(y=..density..),binwidth=I(13000/30), color="orange") + geom_density(alpha=.2, fill="#FF6666")

#summary(pop$shares,na.rm=TRUE)
#quantile(pop$shares,na.rm=TRUE, probs=seq(0,1,by=.1))
```

This distribution can be approximated with either
a) Weibull, b) gamma distribution, or c) Poisson

In an effort to fit a distribution that most closely approximates the data on number of hits, we use ecdf - empirical cumulative distribution function.

Can we use qqplot to compare theoretical distributions to the observed?

Doing so may require a new dataframe/vector of samples from the theoretical population:
a) Weibull requires a shape and scale parameter

```{r}
summary(pop[,39])
head(pop[,39])
```

Convert categorical/factor variables from wide to long using the reshape2 package. Try using URL as the time variable... if that doesn't work, I will have to create such a variable.

Using melt, cast and dcast from the reshape2 package
```{r}
pop.days<-pop[,c(1,32:38)]
nuevo<-melt(pop.days, id="url") #creates a d.f.

nuevo.2<-nuevo[!nuevo$value==0,]
nuevo.3<-nuevo.2[,-3]
#manipulate the days label
levels(nuevo.3$variable)<-c('monday','tuesday','wednesday','thursday','friday','saturday','sunday')

pop.d<-merge(pop[,-c(32:38)],nuevo.3,by=factor("url"),sort=FALSE)

names(pop.d)[55]<-'day.of.week'
```

Apply the above function to data channel types... converting data channel to a factor variable from a boolean indicator:
```{r}
pop.channels<-pop[,c(1,14:19)]
nuevo.c<-melt(pop.channels, id="url") #creates a d.f.

nuevo[as.character(factor(nuevo$url))=='http://mashable.com/2013/01/07/amazon-instant-video-browser/',]

nuevo[factor(nuevo$url)=='http://mashable.com/2013/01/07/amazon-instant-video-browser/',]

nuevo.c.2<-nuevo.c[!nuevo.c$value==0,]
```

Unfortunately, we have no channel data for 6,134 news articles. Regressing on this variable means testing on a truncated data set. There are still 33,510 observations for the data set with channel data.

```{r}
nuevo.c.3<-nuevo.c.2[,-3]
#manipulate the days label
levels(nuevo.c.3$variable)<-c('lifestyle','entertainment','bus','socmed','tech','world')

pop.d.c<-merge(pop.d[,-c(14:19)],nuevo.c.3,by=factor("url"),sort=FALSE)

names(pop.d.c)[50]<-'data.channel'
```
# Appendix

```{r}

opts_chunk$set(include = F)

```

### Source

UCI Machine Learning Repository - Online News Popularity Data Set, donated 5/31/15
https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity

### Citation

    K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision
    Support System for Predicting the Popularity of Online News. Proceedings
    of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence,
    September, Coimbra, Portugal.